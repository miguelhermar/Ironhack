{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b2668c",
   "metadata": {},
   "source": [
    "### Bag of Words Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c85660",
   "metadata": {},
   "source": [
    "Bag of words (BoW) is an important technique in text mining and information retrieval. BoW uses term-frequency vectors to represent the content of text documents which makes it possible to use mathematics and computer programs to analyze and compare text documents.\n",
    "\n",
    "BoW contains the following information:\n",
    "\n",
    "A dictionary of all the terms (words) in the text documents. The terms are normalized in terms of the letter case (e.g. Ironhack => ironhack), tense (e.g. had => have), singular form (e.g. students => student), etc.\n",
    "The number of occurrences of each normalized term in each document.\n",
    "For example, assume we have three text documents:\n",
    "\n",
    "DOC 1: Ironhack is cool.\n",
    "\n",
    "DOC 2: I love Ironhack.\n",
    "\n",
    "DOC 3: I am a student at Ironhack.\n",
    "\n",
    "The BoW of the above documents looks like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130d84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['doc1.txt', 'doc2.txt', 'doc3.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95ca77",
   "metadata": {},
   "source": [
    "Define an empty array corpus that will contain the content strings of the docs. Loop docs and read the content of each doc into the corpus array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42cebeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "corpus = []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4a47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('./'):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(filename, 'r') as f:\n",
    "                content = f.readline()\n",
    "        corpus.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdfb2881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ironhack is cool.', 'I am a student at Ironhack.', 'I love Ironhack.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c654d",
   "metadata": {},
   "source": [
    "Write your code below to process corpus (convert to lower case and remove special characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1373a98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack is cool', 'i am a student at ironhack', 'i love ironhack']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_list = [(lambda i: i.lower().strip('\\.')) (i) for i in corpus]\n",
    "clean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a0a03",
   "metadata": {},
   "source": [
    "Now define bag_of_words as an empty array. It will be used to store the unique terms in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9ce5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a31b1",
   "metadata": {},
   "source": [
    "Loop through corpus. In each loop, do the following:\n",
    "\n",
    "Break the string into an array of terms.\n",
    "Create a sub-loop to iterate the terms array.\n",
    "In each sub-loop, you'll check if the current term is already contained in bag_of_words. If not in bag_of_words, append it to the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb186e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in clean_list:\n",
    "    terms = sentence.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adca8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 'is', 'cool', 'i', 'am', 'a', 'student', 'at', 'love']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sentence in clean_list:\n",
    "    terms = sentence.split(' ')\n",
    "    for word in terms:\n",
    "        if word not in bag_of_words:\n",
    "            bag_of_words.append(word)\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c412c",
   "metadata": {},
   "source": [
    "Now we define an empty array called term_freq. Loop corpus for a second time. In each loop, create a sub-loop to iterate the terms in bag_of_words. Count how many times each term appears in each doc of corpus. Append the term-frequency array to term_freq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5ce1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq = []\n",
    "corpus_split = []\n",
    "\n",
    "for i in clean_list:\n",
    "    corpus_split.append(i.split(' '))\n",
    "\n",
    "for sentence in corpus_split:\n",
    "    sent_vec = []\n",
    "    for word in bag_of_words:\n",
    "        if word in sentence:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    term_freq.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6fe72af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 1, 1, 1, 1, 0],\n",
       " [1, 0, 0, 1, 0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc768bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 0, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Result printed in Ironhack Lab\n",
    "\n",
    "[[1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 0, 0, 0, 0], [1, 0, 0, 1, 0, 1, 1, 1, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859783b",
   "metadata": {},
   "source": [
    "['ironhack is cool'], \n",
    "\n",
    "['i am a student at ironhack'], \n",
    "\n",
    "['i love ironhack'] \n",
    "\n",
    "['ironhack', 'is', 'cool', 'i', 'am', 'a', 'student', 'at', 'love']\n",
    "\n",
    "['a', 'am', 'at', 'cool', 'i', 'ironhack', 'is', 'love', 'student']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107240bc",
   "metadata": {},
   "source": [
    "### Bonus Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc811945",
   "metadata": {},
   "source": [
    "Optimize your solution for the above question by removing stop words from the BoW. For your convenience, a list of stop words is defined for you in the next cell.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Combine all your previous codes to the cell below.\n",
    "Improve your solution by ignoring stop words in bag_of_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e5b0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'fifty', 'four', 'not', 'own', 'through', 'yourselves', 'go', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55cbd879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 'cool', 'student', 'love']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_with_no_stopwords = [word for word in bag_of_words if word not in stop_words]\n",
    "bow_with_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc19a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_2 = []\n",
    "corpus_split_2 = []\n",
    "\n",
    "for i in clean_list:\n",
    "    corpus_split_2.append(i.split(' '))\n",
    "\n",
    "for sentence in corpus_split_2:\n",
    "    sent_vec_2 = []\n",
    "    for word in bow_with_no_stopwords:\n",
    "        if word in sentence:\n",
    "            sent_vec_2.append(1)\n",
    "        else:\n",
    "            sent_vec_2.append(0)\n",
    "    term_freq_2.append(sent_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c78e567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 0, 0], [1, 0, 1, 0], [1, 0, 0, 1]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf74b6",
   "metadata": {},
   "source": [
    "### Additional Challenge for the Nerds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6428e",
   "metadata": {},
   "source": [
    "We will learn Scikit-Learn in Module 3 which has built in the BoW feature. Try to use Scikit-Learn to generate the BoW for this challenge and check whether the output is the same as yours. You will need to do some googling to find out how to use Scikit-Learn to generate BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d46e7de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b334e3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am' 'at' 'cool' 'ironhack' 'is' 'love' 'student']\n",
      "{'ironhack': 3, 'is': 4, 'cool': 2, 'am': 0, 'student': 6, 'at': 1, 'love': 5}\n",
      "[[0 0 1 1 1 0 0]\n",
      " [1 1 0 1 0 0 1]\n",
      " [0 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the bag-of-words model\n",
    "bag = vectorizer.fit_transform(clean_list)\n",
    "\n",
    "# Get unique words / tokens found in all the documents. The unique words / tokens represents\n",
    "# the features\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Associate the indices with each unique word\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# Print the numerical feature vector\n",
    "\n",
    "print(bag.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
